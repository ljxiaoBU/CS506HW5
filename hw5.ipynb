{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import copy\n",
    "import os\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "# nltk.download()\n",
    "# nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import word2vec\n",
    "from gensim import utils\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense, Activation,Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./train.csv', 'r', encoding='utf-8')\n",
    "res_file = open('./test.csv', 'r', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df = pd.read_csv(res_file)\n",
    "res_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(file, sep = ',')\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df['Id']\n",
    "df['new'] = df['Summary']+df['Text']\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('!',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna()\n",
    "df_train = copy.deepcopy(df)\n",
    "\n",
    "df_train = df_train[:460804]\n",
    "df_train['Score'] -= 1\n",
    "# df_train\n",
    "df_test = copy.deepcopy(df)\n",
    "df_test = df_test[460804:]\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df_train['new']\n",
    "review_test = df_test['new']\n",
    "res = df_train['Score']\n",
    "res_test = df_test['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datacleaning():\n",
    "    def constructLabeledSentences(data):\n",
    "        sentences=[]\n",
    "        for index, row in data.iteritems():\n",
    "            sentences.append(LabeledSentence(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n",
    "        return sentences\n",
    "\n",
    "    def textClean(text):\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = text.lower().split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]    \n",
    "        text = \" \".join(text)\n",
    "        return(text)\n",
    "\n",
    "    def cleanup(text):\n",
    "        text = datacleaning.textClean(text)\n",
    "        text= text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = reviews.apply(datacleaning.cleanup)\n",
    "allText_test = review_test.apply(datacleaning.cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljxiao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "sentences = datacleaning.constructLabeledSentences(allText)\n",
    "sentence_test = datacleaning.constructLabeledSentences(allText_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMENSION=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location):\n",
    "    \"\"\"Returns trained word2vec\n",
    "    \n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=INPUT_DIMENSION, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2vmodel not found. training model\n",
      "Model done training. Saving to disk\n"
     ]
    }
   ],
   "source": [
    "w2vec = get_word2vec(MySentences(allText),'w2vmodel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2vmodel_test not found. training model\n",
      "Model done training. Saving to disk\n"
     ]
    }
   ],
   "source": [
    "w2vec_test = get_word2vec(MySentences(allText_test),'w2vmodel_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljxiao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "mean_embedded = mean_embedding_vectorizer.fit_transform(allText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljxiao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "mean_embedding_vectorizer_test = MeanEmbeddingVectorizer(w2vec_test)\n",
    "mean_embedded_test = mean_embedding_vectorizer.fit_transform(allText_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05243551, -0.02769762, -0.06115679, ..., -0.0159556 ,\n",
       "         0.12447834,  0.03116012],\n",
       "       [-0.27977613, -0.24672899, -0.03321983, ..., -0.19077405,\n",
       "         0.13434066, -0.04224192],\n",
       "       [ 0.14574905, -0.19832307,  0.20233639, ...,  0.22789514,\n",
       "        -0.27232435, -0.39158624],\n",
       "       ...,\n",
       "       [ 0.0545247 ,  0.02330562, -0.27810237, ...,  0.17907244,\n",
       "         0.09386668, -0.04984564],\n",
       "       [-0.08122585, -0.0351049 ,  0.18333995, ..., -0.13410072,\n",
       "         0.10611135, -0.29965791],\n",
       "       [-0.21933363,  0.30772182, -0.29920992, ..., -0.09799889,\n",
       "         0.03371126,  0.30878082]])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20045018, -0.21382578, -0.05998511, ...,  0.02013098,\n",
       "         0.06724421, -0.0216706 ],\n",
       "       [-0.04237926, -0.06556096, -0.0665714 , ...,  0.06260543,\n",
       "         0.03746755,  0.03807595],\n",
       "       [ 0.03357392, -0.27137223, -0.48566854, ..., -0.00579256,\n",
       "         0.36872089,  0.03321708],\n",
       "       ...,\n",
       "       [-0.12596942, -0.02834934, -0.15631673, ...,  0.05561934,\n",
       "         0.17696366, -0.0031377 ],\n",
       "       [ 0.16402821,  0.30524489, -0.06436918, ...,  0.0514502 ,\n",
       "        -0.00220442,  0.11592378],\n",
       "       [-0.08391818,  0.03125412, -0.11925033, ..., -0.17400557,\n",
       "         0.14263852, -0.01064425]])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embedded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"mean_embedded.npy\", mean_embedded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"mean_embedded_test.npy\", mean_embedded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = np.load(\"mean_embedded.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_test = np.load(\"mean_embedded_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(arrays,res,test_size=0.1, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414723, 1000)"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46081, 1000)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(Y_train, num_classes=5)\n",
    "y_test = to_categorical(Y_test, num_classes=5)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414723, 5)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=INPUT_DIMENSION,activation =\"relu\"))  #100\n",
    "# model.add(Dropout(0.5))    #0.3\n",
    "model.add(Dense(128,activation =\"relu\"))    #80\n",
    "# model.add(Dropout(0.5))    #0.5\n",
    "model.add(Dense(64,activation =\"relu\"))    #36\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(36,activation =\"relu\"))\n",
    "model.add(Dense(5,activation =\"softmax\"))   #6\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 414723 samples, validate on 46081 samples\n",
      "Epoch 1/30\n",
      "414723/414723 [==============================] - 19s 45us/step - loss: 1.1678 - acc: 0.6321 - val_loss: 1.0620 - val_acc: 0.6368\n",
      "Epoch 2/30\n",
      "414723/414723 [==============================] - 18s 43us/step - loss: 0.9992 - acc: 0.6405 - val_loss: 0.9519 - val_acc: 0.6490\n",
      "Epoch 3/30\n",
      "414723/414723 [==============================] - 18s 43us/step - loss: 0.9196 - acc: 0.6670 - val_loss: 0.9008 - val_acc: 0.6748\n",
      "Epoch 4/30\n",
      "414723/414723 [==============================] - 16s 39us/step - loss: 0.8815 - acc: 0.6812 - val_loss: 0.8709 - val_acc: 0.6809\n",
      "Epoch 5/30\n",
      "414723/414723 [==============================] - 17s 40us/step - loss: 0.8558 - acc: 0.6872 - val_loss: 0.8486 - val_acc: 0.6859\n",
      "Epoch 6/30\n",
      "414723/414723 [==============================] - 18s 43us/step - loss: 0.8362 - acc: 0.6922 - val_loss: 0.8306 - val_acc: 0.6919\n",
      "Epoch 7/30\n",
      "414723/414723 [==============================] - 18s 44us/step - loss: 0.8203 - acc: 0.6970 - val_loss: 0.8158 - val_acc: 0.6963\n",
      "Epoch 8/30\n",
      "414723/414723 [==============================] - 18s 43us/step - loss: 0.8076 - acc: 0.7006 - val_loss: 0.8041 - val_acc: 0.6999\n",
      "Epoch 9/30\n",
      "414723/414723 [==============================] - 17s 41us/step - loss: 0.7975 - acc: 0.7040 - val_loss: 0.7947 - val_acc: 0.7030\n",
      "Epoch 10/30\n",
      "414723/414723 [==============================] - 17s 40us/step - loss: 0.7894 - acc: 0.7067 - val_loss: 0.7872 - val_acc: 0.7055\n",
      "Epoch 11/30\n",
      "414723/414723 [==============================] - 16s 39us/step - loss: 0.7828 - acc: 0.7088 - val_loss: 0.7808 - val_acc: 0.7074\n",
      "Epoch 12/30\n",
      "414723/414723 [==============================] - 18s 43us/step - loss: 0.7774 - acc: 0.7102 - val_loss: 0.7757 - val_acc: 0.7092\n",
      "Epoch 13/30\n",
      "414723/414723 [==============================] - 17s 40us/step - loss: 0.7728 - acc: 0.7117 - val_loss: 0.7715 - val_acc: 0.7110\n",
      "Epoch 14/30\n",
      "414723/414723 [==============================] - 19s 47us/step - loss: 0.7689 - acc: 0.7130 - val_loss: 0.7678 - val_acc: 0.7120\n",
      "Epoch 15/30\n",
      "414723/414723 [==============================] - 18s 44us/step - loss: 0.7655 - acc: 0.7140 - val_loss: 0.7646 - val_acc: 0.7129\n",
      "Epoch 16/30\n",
      "414723/414723 [==============================] - 18s 44us/step - loss: 0.7625 - acc: 0.7149 - val_loss: 0.7618 - val_acc: 0.7142\n",
      "Epoch 17/30\n",
      "414723/414723 [==============================] - 17s 42us/step - loss: 0.7599 - acc: 0.7158 - val_loss: 0.7593 - val_acc: 0.7158\n",
      "Epoch 18/30\n",
      "414723/414723 [==============================] - 18s 42us/step - loss: 0.7575 - acc: 0.7167 - val_loss: 0.7574 - val_acc: 0.7157\n",
      "Epoch 19/30\n",
      "414723/414723 [==============================] - 18s 43us/step - loss: 0.7554 - acc: 0.7174 - val_loss: 0.7553 - val_acc: 0.7158\n",
      "Epoch 20/30\n",
      "414723/414723 [==============================] - 17s 41us/step - loss: 0.7534 - acc: 0.7180 - val_loss: 0.7536 - val_acc: 0.7167\n",
      "Epoch 21/30\n",
      "414723/414723 [==============================] - 16s 39us/step - loss: 0.7517 - acc: 0.7186 - val_loss: 0.7518 - val_acc: 0.7173\n",
      "Epoch 22/30\n",
      "414723/414723 [==============================] - 16s 39us/step - loss: 0.7500 - acc: 0.7192 - val_loss: 0.7503 - val_acc: 0.7178\n",
      "Epoch 23/30\n",
      "414723/414723 [==============================] - 16s 39us/step - loss: 0.7484 - acc: 0.7198 - val_loss: 0.7488 - val_acc: 0.7191\n",
      "Epoch 24/30\n",
      "414723/414723 [==============================] - 16s 39us/step - loss: 0.7469 - acc: 0.7202 - val_loss: 0.7477 - val_acc: 0.7191\n",
      "Epoch 25/30\n",
      "414723/414723 [==============================] - 17s 41us/step - loss: 0.7455 - acc: 0.7207 - val_loss: 0.7463 - val_acc: 0.7191\n",
      "Epoch 26/30\n",
      "414723/414723 [==============================] - 17s 42us/step - loss: 0.7442 - acc: 0.7209 - val_loss: 0.7450 - val_acc: 0.7200\n",
      "Epoch 27/30\n",
      "414723/414723 [==============================] - 16s 39us/step - loss: 0.7430 - acc: 0.7215 - val_loss: 0.7443 - val_acc: 0.7198\n",
      "Epoch 28/30\n",
      "414723/414723 [==============================] - 16s 39us/step - loss: 0.7418 - acc: 0.7216 - val_loss: 0.7430 - val_acc: 0.7203\n",
      "Epoch 29/30\n",
      "414723/414723 [==============================] - 16s 39us/step - loss: 0.7407 - acc: 0.7220 - val_loss: 0.7418 - val_acc: 0.7211\n",
      "Epoch 30/30\n",
      "414723/414723 [==============================] - 17s 42us/step - loss: 0.7395 - acc: 0.7226 - val_loss: 0.7410 - val_acc: 0.7207\n",
      "Test loss: 0.7409867554335842\n",
      "Test accuracy: 0.7207308869165165\n"
     ]
    }
   ],
   "source": [
    "clf = model.fit(X_train, y_train,\n",
    "                batch_size=2048,   #20\n",
    "                epochs=30,\n",
    "                verbose=1,\n",
    "                validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=model.predict(X_test)\n",
    "pre=[]\n",
    "for i in a:\n",
    "    bb =i.tolist()\n",
    "    pre.append(bb.index(max(bb)))\n",
    "sum = 0\n",
    "for j in range(len(pre)):\n",
    "    if pre[j] == Y_test.values[j]:\n",
    "        sum+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7207308869165165"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum/len(pre)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = model.predict(arrays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.0799391e-01, 5.2932505e-02, 1.4453160e-02, 5.5721523e-03,\n",
       "        1.9048342e-02],\n",
       "       [8.6871393e-02, 1.4496094e-01, 2.4393438e-01, 2.5009528e-01,\n",
       "        2.7413800e-01],\n",
       "       [3.4338798e-05, 5.1569956e-04, 1.4549427e-02, 3.1856743e-01,\n",
       "        6.6633308e-01],\n",
       "       ...,\n",
       "       [2.2258326e-02, 1.6062679e-02, 2.2833383e-02, 8.5690252e-02,\n",
       "        8.5315531e-01],\n",
       "       [1.8802394e-04, 1.9296582e-04, 9.3547098e-04, 2.4127211e-02,\n",
       "        9.7455639e-01],\n",
       "       [1.6060630e-03, 6.0607528e-04, 1.7593731e-03, 2.3677042e-02,\n",
       "        9.7235143e-01]], dtype=float32)"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_score = []\n",
    "for i in result_test:\n",
    "    tmp =i.tolist()\n",
    "    result_score.append(tmp.index(max(tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(result_score)):\n",
    "    result_score[i] += 1\n",
    "# result_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = copy.deepcopy(result_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df['Score'] = final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "return_df.to_csv(\"predictedScore.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,max_depth=16,verbose=1).fit(X_train, y_train)\n",
    "yy = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, yy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = XGBClassifier(max_depth=5, learning_rate=0.05,n_estimators=100,verbose=1).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy1 = clf1.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in Y_test:\n",
    "    lst.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "ar = array(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst2 = []\n",
    "for i in yy1:\n",
    "    lst2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar2 = array(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {}'.format(accuracy_score(ar, ar2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
